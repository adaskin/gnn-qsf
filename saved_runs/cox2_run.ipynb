{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'COX2', 'n_layers': 4, 'hidden_dims': [32, 16], 'batch_size': 32, 'lr': 0.01, 'n_splits': 10, 'val_ratio': 0.1, 'epochs': 50, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/COX2.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([378, 4096])\n",
      "train_phases shape: torch.Size([378, 12, 12])\n",
      "train_labs shape: torch.Size([378])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.5758 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.4898 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4798 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4592 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4008 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.4191 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 007 | Train Loss 0.3739 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 7]\n",
      "[COX2] Epoch 008 | Train Loss 0.3714 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 009 | Train Loss 0.3419 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 010 | Train Loss 0.3109 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 011 | Train Loss 0.3047 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 012 | Train Loss 0.2672 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 013 | Train Loss 0.2874 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 014 | Train Loss 0.2622 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 015 | Train Loss 0.2425 | Val Acc 0.6190 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 016 | Train Loss 0.2377 | Val Acc 0.6190 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 017 | Train Loss 0.2507 | Val Acc 0.6429 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 018 | Train Loss 0.2086 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 019 | Train Loss 0.2426 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 020 | Train Loss 0.1929 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 021 | Train Loss 0.2580 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 022 | Train Loss 0.2510 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 023 | Train Loss 0.2546 | Val Acc 0.6429 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 024 | Train Loss 0.1766 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 025 | Train Loss 0.1889 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 026 | Train Loss 0.1393 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 027 | Train Loss 0.1584 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 028 | Train Loss 0.1399 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 029 | Train Loss 0.1502 | Val Acc 0.7143 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 030 | Train Loss 0.1442 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 031 | Train Loss 0.1285 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 032 | Train Loss 0.1361 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 033 | Train Loss 0.1158 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "[COX2] Epoch 034 | Train Loss 0.1057 | Val Acc 0.7381 | LR 1.0e-03 [Best Val Acc: 0.8095 @ Epoch 20]\n",
      "Early stop @ epoch 35 | Best val_acc=0.8095\n",
      "[COX2] Fold Test Acc: 0.7872\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([378, 4096])\n",
      "train_phases shape: torch.Size([378, 12, 12])\n",
      "train_labs shape: torch.Size([378])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.5958 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.5062 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4965 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4605 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4126 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.4004 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 6]\n",
      "[COX2] Epoch 007 | Train Loss 0.3823 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 7]\n",
      "[COX2] Epoch 008 | Train Loss 0.3640 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 7]\n",
      "[COX2] Epoch 009 | Train Loss 0.3694 | Val Acc 0.5714 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 7]\n",
      "[COX2] Epoch 010 | Train Loss 0.3251 | Val Acc 0.8810 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 10]\n",
      "[COX2] Epoch 011 | Train Loss 0.3089 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 10]\n",
      "[COX2] Epoch 012 | Train Loss 0.2777 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 10]\n",
      "[COX2] Epoch 013 | Train Loss 0.2809 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 10]\n",
      "[COX2] Epoch 014 | Train Loss 0.2907 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 10]\n",
      "[COX2] Epoch 015 | Train Loss 0.2357 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 10]\n",
      "[COX2] Epoch 016 | Train Loss 0.2310 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 10]\n",
      "[COX2] Epoch 017 | Train Loss 0.2817 | Val Acc 0.8810 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 018 | Train Loss 0.2492 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 019 | Train Loss 0.2287 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 020 | Train Loss 0.2378 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 021 | Train Loss 0.1865 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 022 | Train Loss 0.2262 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 023 | Train Loss 0.1959 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 024 | Train Loss 0.2019 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 025 | Train Loss 0.2088 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 026 | Train Loss 0.2184 | Val Acc 0.7143 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 17]\n",
      "[COX2] Epoch 027 | Train Loss 0.1450 | Val Acc 0.8810 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 27]\n",
      "[COX2] Epoch 028 | Train Loss 0.1351 | Val Acc 0.8810 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 029 | Train Loss 0.1366 | Val Acc 0.8810 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 030 | Train Loss 0.1359 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 031 | Train Loss 0.1069 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 032 | Train Loss 0.1375 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 033 | Train Loss 0.1233 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 034 | Train Loss 0.1168 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 035 | Train Loss 0.1222 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 036 | Train Loss 0.1074 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 037 | Train Loss 0.1316 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 038 | Train Loss 0.1192 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 039 | Train Loss 0.1031 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 040 | Train Loss 0.1132 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 041 | Train Loss 0.0986 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "[COX2] Epoch 042 | Train Loss 0.1190 | Val Acc 0.8571 | LR 1.0e-04 [Best Val Acc: 0.8810 @ Epoch 28]\n",
      "Early stop @ epoch 43 | Best val_acc=0.8810\n",
      "[COX2] Fold Test Acc: 0.7660\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([378, 4096])\n",
      "train_phases shape: torch.Size([378, 12, 12])\n",
      "train_labs shape: torch.Size([378])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.5829 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.5006 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4721 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4488 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.3901 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.3654 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 007 | Train Loss 0.3378 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 008 | Train Loss 0.2946 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 009 | Train Loss 0.3335 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 010 | Train Loss 0.2755 | Val Acc 0.6190 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 011 | Train Loss 0.2911 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 012 | Train Loss 0.2603 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 013 | Train Loss 0.2485 | Val Acc 0.6429 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 014 | Train Loss 0.2323 | Val Acc 0.6429 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 015 | Train Loss 0.2120 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 15]\n",
      "[COX2] Epoch 016 | Train Loss 0.2142 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 15]\n",
      "[COX2] Epoch 017 | Train Loss 0.2409 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 15]\n",
      "[COX2] Epoch 018 | Train Loss 0.2038 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 18]\n",
      "[COX2] Epoch 019 | Train Loss 0.1490 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 19]\n",
      "[COX2] Epoch 020 | Train Loss 0.1661 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 19]\n",
      "[COX2] Epoch 021 | Train Loss 0.1597 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 19]\n",
      "[COX2] Epoch 022 | Train Loss 0.1832 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 19]\n",
      "[COX2] Epoch 023 | Train Loss 0.1656 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 19]\n",
      "[COX2] Epoch 024 | Train Loss 0.1170 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 025 | Train Loss 0.1491 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 026 | Train Loss 0.1347 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 027 | Train Loss 0.1184 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 028 | Train Loss 0.1333 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 029 | Train Loss 0.1467 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 030 | Train Loss 0.1130 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 031 | Train Loss 0.1119 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 032 | Train Loss 0.1265 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 033 | Train Loss 0.1071 | Val Acc 0.7619 | LR 1.0e-04 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 034 | Train Loss 0.1042 | Val Acc 0.7619 | LR 1.0e-04 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 035 | Train Loss 0.1218 | Val Acc 0.7619 | LR 1.0e-04 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 036 | Train Loss 0.1045 | Val Acc 0.7619 | LR 1.0e-04 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 037 | Train Loss 0.1232 | Val Acc 0.7619 | LR 1.0e-04 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "[COX2] Epoch 038 | Train Loss 0.1169 | Val Acc 0.7619 | LR 1.0e-04 [Best Val Acc: 0.7857 @ Epoch 24]\n",
      "Early stop @ epoch 39 | Best val_acc=0.7857\n",
      "[COX2] Fold Test Acc: 0.7234\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([378, 4096])\n",
      "train_phases shape: torch.Size([378, 12, 12])\n",
      "train_labs shape: torch.Size([378])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.5797 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.4918 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4764 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4476 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4217 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.4219 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 007 | Train Loss 0.3807 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 008 | Train Loss 0.3556 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 8]\n",
      "[COX2] Epoch 009 | Train Loss 0.3528 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 9]\n",
      "[COX2] Epoch 010 | Train Loss 0.3156 | Val Acc 0.6667 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 9]\n",
      "[COX2] Epoch 011 | Train Loss 0.3033 | Val Acc 0.5476 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 9]\n",
      "[COX2] Epoch 012 | Train Loss 0.2698 | Val Acc 0.6190 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 9]\n",
      "[COX2] Epoch 013 | Train Loss 0.2650 | Val Acc 0.6190 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 9]\n",
      "[COX2] Epoch 014 | Train Loss 0.2890 | Val Acc 0.5476 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 9]\n",
      "[COX2] Epoch 015 | Train Loss 0.2297 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 9]\n",
      "[COX2] Epoch 016 | Train Loss 0.2084 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 16]\n",
      "[COX2] Epoch 017 | Train Loss 0.2408 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.7857 @ Epoch 16]\n",
      "[COX2] Epoch 018 | Train Loss 0.1936 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 18]\n",
      "[COX2] Epoch 019 | Train Loss 0.1480 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 19]\n",
      "[COX2] Epoch 020 | Train Loss 0.1591 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 19]\n",
      "[COX2] Epoch 021 | Train Loss 0.1790 | Val Acc 0.8571 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 022 | Train Loss 0.1749 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 023 | Train Loss 0.1620 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 024 | Train Loss 0.1609 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 025 | Train Loss 0.1457 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 026 | Train Loss 0.1182 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 027 | Train Loss 0.1593 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 028 | Train Loss 0.1116 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 029 | Train Loss 0.1203 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 030 | Train Loss 0.1135 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 031 | Train Loss 0.1217 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 032 | Train Loss 0.1285 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 033 | Train Loss 0.1050 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 034 | Train Loss 0.1006 | Val Acc 0.8095 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "[COX2] Epoch 035 | Train Loss 0.1114 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 21]\n",
      "Early stop @ epoch 36 | Best val_acc=0.8571\n",
      "[COX2] Fold Test Acc: 0.7234\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([378, 4096])\n",
      "train_phases shape: torch.Size([378, 12, 12])\n",
      "train_labs shape: torch.Size([378])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.5772 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.4961 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4974 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 004 | Train Loss 0.4447 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.3967 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.3876 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 6]\n",
      "[COX2] Epoch 007 | Train Loss 0.3631 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 6]\n",
      "[COX2] Epoch 008 | Train Loss 0.3555 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 8]\n",
      "[COX2] Epoch 009 | Train Loss 0.3479 | Val Acc 0.9048 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 010 | Train Loss 0.3011 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 011 | Train Loss 0.3194 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 012 | Train Loss 0.2816 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 013 | Train Loss 0.2912 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 014 | Train Loss 0.2497 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 015 | Train Loss 0.2247 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 016 | Train Loss 0.2617 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 017 | Train Loss 0.2097 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 018 | Train Loss 0.2496 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 019 | Train Loss 0.1855 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 020 | Train Loss 0.2618 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 021 | Train Loss 0.1924 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 022 | Train Loss 0.2543 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "[COX2] Epoch 023 | Train Loss 0.1744 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.9048 @ Epoch 9]\n",
      "Early stop @ epoch 24 | Best val_acc=0.9048\n",
      "[COX2] Fold Test Acc: 0.8511\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([378, 4096])\n",
      "train_phases shape: torch.Size([378, 12, 12])\n",
      "train_labs shape: torch.Size([378])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.5963 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.5073 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4736 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4306 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4343 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 006 | Train Loss 0.3969 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 6]\n",
      "[COX2] Epoch 007 | Train Loss 0.3595 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 6]\n",
      "[COX2] Epoch 008 | Train Loss 0.3630 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 8]\n",
      "[COX2] Epoch 009 | Train Loss 0.3197 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 010 | Train Loss 0.3047 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 011 | Train Loss 0.2825 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 012 | Train Loss 0.2935 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 013 | Train Loss 0.2843 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 014 | Train Loss 0.2350 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 015 | Train Loss 0.2311 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 016 | Train Loss 0.2501 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 017 | Train Loss 0.2903 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 018 | Train Loss 0.2587 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 019 | Train Loss 0.2096 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 020 | Train Loss 0.2057 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 021 | Train Loss 0.1804 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 022 | Train Loss 0.2538 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "[COX2] Epoch 023 | Train Loss 0.1925 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 9]\n",
      "Early stop @ epoch 24 | Best val_acc=0.8095\n",
      "[COX2] Fold Test Acc: 0.7021\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([378, 4096])\n",
      "train_phases shape: torch.Size([378, 12, 12])\n",
      "train_labs shape: torch.Size([378])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.5936 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.4955 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4674 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4195 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4099 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.3808 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 6]\n",
      "[COX2] Epoch 007 | Train Loss 0.3642 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 6]\n",
      "[COX2] Epoch 008 | Train Loss 0.3336 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 6]\n",
      "[COX2] Epoch 009 | Train Loss 0.3346 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 6]\n",
      "[COX2] Epoch 010 | Train Loss 0.3101 | Val Acc 0.8571 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 011 | Train Loss 0.2632 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 012 | Train Loss 0.2658 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 013 | Train Loss 0.2443 | Val Acc 0.6667 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 014 | Train Loss 0.2445 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 015 | Train Loss 0.2548 | Val Acc 0.6667 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 016 | Train Loss 0.2114 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 017 | Train Loss 0.2576 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 018 | Train Loss 0.2048 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 019 | Train Loss 0.1998 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 020 | Train Loss 0.2304 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 021 | Train Loss 0.2099 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 022 | Train Loss 0.2312 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 023 | Train Loss 0.1749 | Val Acc 0.8095 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "[COX2] Epoch 024 | Train Loss 0.1600 | Val Acc 0.8095 | LR 1.0e-03 [Best Val Acc: 0.8571 @ Epoch 10]\n",
      "Early stop @ epoch 25 | Best val_acc=0.8571\n",
      "[COX2] Fold Test Acc: 0.8298\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([379, 4096])\n",
      "train_phases shape: torch.Size([379, 12, 12])\n",
      "train_labs shape: torch.Size([379])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.6184 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.5217 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4856 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4438 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4524 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 006 | Train Loss 0.3712 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 007 | Train Loss 0.3660 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 008 | Train Loss 0.3523 | Val Acc 0.5476 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 009 | Train Loss 0.2826 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 010 | Train Loss 0.2800 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 011 | Train Loss 0.3009 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 012 | Train Loss 0.2154 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 013 | Train Loss 0.2668 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 014 | Train Loss 0.2582 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 015 | Train Loss 0.2407 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 016 | Train Loss 0.2247 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 017 | Train Loss 0.2341 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 018 | Train Loss 0.2256 | Val Acc 0.6667 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 019 | Train Loss 0.2151 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 020 | Train Loss 0.1783 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 021 | Train Loss 0.1639 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 022 | Train Loss 0.1926 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 023 | Train Loss 0.1815 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 024 | Train Loss 0.1587 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 025 | Train Loss 0.2321 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 026 | Train Loss 0.1757 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 027 | Train Loss 0.1835 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 028 | Train Loss 0.1582 | Val Acc 0.5238 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 029 | Train Loss 0.1765 | Val Acc 0.4524 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 030 | Train Loss 0.1235 | Val Acc 0.5476 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "[COX2] Epoch 031 | Train Loss 0.1213 | Val Acc 0.6667 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 17]\n",
      "Early stop @ epoch 32 | Best val_acc=0.8095\n",
      "[COX2] Fold Test Acc: 0.7391\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([379, 4096])\n",
      "train_phases shape: torch.Size([379, 12, 12])\n",
      "train_labs shape: torch.Size([379])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.6092 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.5238 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4712 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4420 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4340 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.4117 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 6]\n",
      "[COX2] Epoch 007 | Train Loss 0.3662 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 008 | Train Loss 0.3412 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 009 | Train Loss 0.3172 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 010 | Train Loss 0.3335 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 011 | Train Loss 0.3065 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 11]\n",
      "[COX2] Epoch 012 | Train Loss 0.2351 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 11]\n",
      "[COX2] Epoch 013 | Train Loss 0.2465 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 11]\n",
      "[COX2] Epoch 014 | Train Loss 0.2546 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 11]\n",
      "[COX2] Epoch 015 | Train Loss 0.2403 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 11]\n",
      "[COX2] Epoch 016 | Train Loss 0.2489 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 017 | Train Loss 0.2666 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 018 | Train Loss 0.2398 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 019 | Train Loss 0.2415 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 020 | Train Loss 0.1978 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 021 | Train Loss 0.1741 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 022 | Train Loss 0.1684 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 023 | Train Loss 0.1540 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 024 | Train Loss 0.1739 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 025 | Train Loss 0.1795 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 026 | Train Loss 0.1730 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 027 | Train Loss 0.1936 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 028 | Train Loss 0.1616 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 16]\n",
      "[COX2] Epoch 029 | Train Loss 0.1681 | Val Acc 0.8333 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 030 | Train Loss 0.1869 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 031 | Train Loss 0.1881 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 032 | Train Loss 0.1579 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 033 | Train Loss 0.1372 | Val Acc 0.7619 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 034 | Train Loss 0.1393 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 035 | Train Loss 0.1448 | Val Acc 0.8095 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 036 | Train Loss 0.1065 | Val Acc 0.8095 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 037 | Train Loss 0.1247 | Val Acc 0.8095 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 29]\n",
      "[COX2] Epoch 038 | Train Loss 0.1094 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 38]\n",
      "[COX2] Epoch 039 | Train Loss 0.1176 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 38]\n",
      "[COX2] Epoch 040 | Train Loss 0.1196 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 38]\n",
      "[COX2] Epoch 041 | Train Loss 0.0945 | Val Acc 0.8333 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 042 | Train Loss 0.0921 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 043 | Train Loss 0.1011 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 044 | Train Loss 0.1109 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 045 | Train Loss 0.0876 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 046 | Train Loss 0.0864 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 047 | Train Loss 0.0718 | Val Acc 0.7857 | LR 1.0e-03 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 048 | Train Loss 0.0936 | Val Acc 0.7857 | LR 1.0e-04 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 049 | Train Loss 0.0931 | Val Acc 0.7857 | LR 1.0e-04 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Epoch 050 | Train Loss 0.0850 | Val Acc 0.7857 | LR 1.0e-04 [Best Val Acc: 0.8333 @ Epoch 41]\n",
      "[COX2] Fold Test Acc: 0.7391\n",
      "\n",
      "number of qubits 12\n",
      "number of nodes 56\n",
      "number of features 38\n",
      "train_feats shape: torch.Size([379, 4096])\n",
      "train_phases shape: torch.Size([379, 12, 12])\n",
      "train_labs shape: torch.Size([379])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1650\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COX2] Epoch 001 | Train Loss 0.6059 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 1]\n",
      "[COX2] Epoch 002 | Train Loss 0.5108 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 2]\n",
      "[COX2] Epoch 003 | Train Loss 0.4739 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 3]\n",
      "[COX2] Epoch 004 | Train Loss 0.4481 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 4]\n",
      "[COX2] Epoch 005 | Train Loss 0.4256 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 006 | Train Loss 0.4139 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 5]\n",
      "[COX2] Epoch 007 | Train Loss 0.3565 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.7857 @ Epoch 7]\n",
      "[COX2] Epoch 008 | Train Loss 0.3396 | Val Acc 0.8095 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 009 | Train Loss 0.2952 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 010 | Train Loss 0.3055 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 011 | Train Loss 0.3170 | Val Acc 0.7143 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 012 | Train Loss 0.2655 | Val Acc 0.6429 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 013 | Train Loss 0.2850 | Val Acc 0.6667 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 014 | Train Loss 0.2543 | Val Acc 0.7857 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 015 | Train Loss 0.2093 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 016 | Train Loss 0.2391 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 017 | Train Loss 0.2215 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 018 | Train Loss 0.2195 | Val Acc 0.6905 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 019 | Train Loss 0.2475 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 020 | Train Loss 0.1866 | Val Acc 0.7619 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 021 | Train Loss 0.1789 | Val Acc 0.7381 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "[COX2] Epoch 022 | Train Loss 0.2015 | Val Acc 0.6429 | LR 1.0e-02 [Best Val Acc: 0.8095 @ Epoch 8]\n",
      "Early stop @ epoch 23 | Best val_acc=0.8095\n",
      "[COX2] Fold Test Acc: 0.6739\n",
      "\n",
      "\n",
      "=== Cross-Validation Results for COX2 ===\n",
      "Per-fold Accuracies: [0.7872340425531915, 0.7659574468085106, 0.723404255319149, 0.723404255319149, 0.851063829787234, 0.7021276595744681, 0.8297872340425532, 0.7391304347826086, 0.7391304347826086, 0.6739130434782609]\n",
      "Mean Test Acc: 0.7535  0.0528\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch_geometric.datasets import TUDataset, MNISTSuperpixels, MoleculeNet\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # limit OpenMP threads for Pennylane\n",
    "import pennylane as qml\n",
    "import warnings\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Quantum Circuit Module\n",
    "# ========================\n",
    "class QuantumCircuit(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = 2**n_qubits\n",
    "\n",
    "        # PennyLane device\n",
    "        self.dev = qml.device(\"lightning.qubit\", wires=n_qubits, batch_obs=True)\n",
    "\n",
    "        # count of rotation & QFT params per layer\n",
    "        n_pairs = n_qubits * (n_qubits - 1) // 2\n",
    "        self.rot_params_per_layer = n_qubits + n_pairs\n",
    "        self.qft_params_per_layer = n_pairs\n",
    "\n",
    "        # trainable parameters (float32 by default)\n",
    "        self.ry_thetas = nn.Parameter(torch.rand(n_layers, self.rot_params_per_layer))\n",
    "        self.qft_phases = nn.Parameter(torch.rand(n_layers, self.qft_params_per_layer))\n",
    "\n",
    "        # Torchbacked QNode\n",
    "        self.qnode = qml.QNode(\n",
    "            self.circuit, self.dev, interface=\"torch\", diff_method=\"adjoint\"\n",
    "        )\n",
    "\n",
    "    def rotation_layer(self, thetas, qubit_connections):\n",
    "        idx = 0\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(thetas[idx], wires=i)\n",
    "            idx += 1\n",
    "        for c in range(self.n_qubits):\n",
    "            for t in range(c + 1, self.n_qubits):\n",
    "                if qubit_connections[c, t] != 0:\n",
    "                    angle = thetas[idx]\n",
    "                    qml.CRY(angle, wires=[c, t])\n",
    "                idx += 1\n",
    "\n",
    "    def parameterized_qft(self, phases, qubit_connections):\n",
    "        pidx = 0\n",
    "        for tgt in range(self.n_qubits):\n",
    "            qml.Hadamard(wires=tgt)\n",
    "            for ctrl in range(tgt + 1, self.n_qubits):\n",
    "                if qubit_connections[ctrl, tgt] != 0:\n",
    "                    angle = phases[pidx] * 0.9 + 0.1 * qubit_connections[ctrl, tgt]\n",
    "                    qml.CRZ(angle, wires=[ctrl, tgt])\n",
    "                pidx += 1\n",
    "\n",
    "    def circuit(self, input_state, qubit_connections):\n",
    "        # amplitude embedding of a 2**n_qubits vector\n",
    "        norm = torch.norm(input_state)\n",
    "        if torch.isclose(norm, torch.tensor(0.0)):\n",
    "            warnings.warn(\" Zerovector embedding: input to quantum circuit is 0!\")\n",
    "\n",
    "            qml.AmplitudeEmbedding(\n",
    "                input_state + 1e-8, wires=range(self.n_qubits), normalize=True\n",
    "            )\n",
    "        else:\n",
    "            qml.AmplitudeEmbedding(\n",
    "                input_state, wires=range(self.n_qubits), normalize=True\n",
    "            )\n",
    "\n",
    "        for layer in range(self.n_layers):\n",
    "            self.rotation_layer(self.ry_thetas[layer], qubit_connections)\n",
    "            self.parameterized_qft(self.qft_phases[layer], qubit_connections)\n",
    "\n",
    "        # measure PauliZ on each qubit\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "    def forward(self, input_state, qubit_connections):\n",
    "        # print(input_state)\n",
    "        raw_output = self.qnode(input_state, qubit_connections)\n",
    "        # Normalize and clip outputs to prevent explosion\n",
    "        output = torch.stack(raw_output).float()  # Ensure float32\n",
    "        # print(output)\n",
    "        # output = torch.clamp(output, -1.0, 1.0)  # Constrain to [-1,1]\n",
    "        return output  #\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Quantum GNN Module\n",
    "# ========================\n",
    "class QuantumGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_qubits,\n",
    "        n_layers=1,\n",
    "        hidden_dims=[64, 32],\n",
    "        output_dim=1,\n",
    "        dropout_prob=0.25,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "        # quantum backbone\n",
    "        self.quantum_circuit = QuantumCircuit(n_qubits, n_layers)\n",
    "\n",
    "        # classical head takes exactly n_qubits inputs\n",
    "        layers = []\n",
    "        input_dim = n_qubits\n",
    "        for h in hidden_dims:\n",
    "            layers += [\n",
    "                nn.Linear(input_dim, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_prob),\n",
    "            ]\n",
    "            input_dim = h\n",
    "\n",
    "        # final classifier\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.classical_net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, features):\n",
    "        padded_features, qubit_connections = (\n",
    "            features  # shapes: [B, 2**n_qubits], [B, n_qubits, n_qubits]\n",
    "        )\n",
    "        batch_size = padded_features.size(0)\n",
    "\n",
    "        # run each graph through the QNode\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            oq = self.quantum_circuit(padded_features[i], qubit_connections[i])\n",
    "            # stack the n_qubit expectation values  [n_qubits]\n",
    "            # quantum_outputs.append(torch.stack(oq))\n",
    "            quantum_outputs.append(oq)\n",
    "\n",
    "        #  [B, n_qubits] but ensure float32\n",
    "        probs = torch.stack(quantum_outputs, dim=0).float()\n",
    "\n",
    "        # feed classical head  logits\n",
    "        return self.classical_net(probs)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Data Preprocessing\n",
    "# ========================\n",
    "class GraphPreprocessor:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.max_nodes = max(d.num_nodes for d in dataset)\n",
    "        self.feature_dim = dataset.num_node_features\n",
    "        if self.feature_dim == 0:\n",
    "            self.feature_dim = 1\n",
    "        self.n_qubits = math.ceil(math.log2(self.max_nodes * self.feature_dim))\n",
    "        self.state_dim = 2**self.n_qubits\n",
    "        self.bin_repr = self.create_bin_repr()  # [max_nodes, n_qubits] float32\n",
    "\n",
    "    def create_bin_repr(self):\n",
    "        nodes = torch.arange(self.max_nodes)\n",
    "        # bit-shift into n_qubits columns, then float32\n",
    "        br = (nodes.unsqueeze(1) >> torch.arange(self.n_qubits - 1, -1, -1)) & 1\n",
    "        return br.float()\n",
    "\n",
    "    # def compute_qubit_connections(self, adj):\n",
    "    #     # adj: [max_nodes, max_nodes] float32, no selfloops\n",
    "    #     a = adj.clone() / adj.shape[0]  # normalize by node count\n",
    "\n",
    "    #     a.fill_diagonal_(0)\n",
    "    #     #  [n_qubits, n_qubits]\n",
    "    #     return torch.einsum(\"ij,ic,jt->ct\", a, self.bin_repr, self.bin_repr)\n",
    "\n",
    "    # MORE ROBUST PHASE INITIALIZATION\n",
    "    def compute_qubit_connections(\n",
    "        self, adj_matrix, n_qubits=None, base_phase=0.1, noise=0.01\n",
    "    ):\n",
    "        N = adj_matrix.shape[0]\n",
    "        if n_qubits is None:\n",
    "            n_qubits = self.n_qubits\n",
    "\n",
    "        # Use precomputed binary representation\n",
    "        node_bin = self.bin_repr[:N].to(adj_matrix.device)  # Only use first N nodes\n",
    "\n",
    "        # Create off-diagonal mask\n",
    "        off_diag_mask = ~torch.eye(N, dtype=torch.bool, device=adj_matrix.device)\n",
    "\n",
    "        # Prepare weight matrix\n",
    "        W = adj_matrix / N\n",
    "        zero_off_diag = (W == 0) & off_diag_mask\n",
    "        W = torch.where(zero_off_diag, base_phase, W)\n",
    "        W = W * off_diag_mask.float()  # Zero out diagonal\n",
    "\n",
    "        # Compute phase matrix using vectorized operations\n",
    "        phase_matrix = node_bin.t() @ (W @ node_bin)\n",
    "\n",
    "        # Add noise to break symmetry\n",
    "        phase_matrix += noise * torch.randn_like(phase_matrix)\n",
    "\n",
    "        return phase_matrix\n",
    "\n",
    "    def preprocess_graph(self, data):\n",
    "        # flatten & pad\n",
    "        # 1) detect missing or zero-width features\n",
    "        if (\n",
    "            (not hasattr(data, \"x\"))\n",
    "            or data.x == None\n",
    "            or data.x.numel() == 0\n",
    "            or data.x.size(1) == 0\n",
    "        ):\n",
    "            \n",
    "            # ensure you modify the same Data object\n",
    "            warnings.warn(\" No node features found, using ones.\")\n",
    "            data.x = torch.ones(self.state_dim, 1, dtype=torch.float32)\n",
    "\n",
    "        # 2) flatten and pad\n",
    "        flat = data.x.reshape(-1).float()  # length = num_nodes*feat_dim\n",
    "\n",
    "        # Check for NaN/Inf in features\n",
    "        if torch.isnan(flat).any() or torch.isinf(flat).any():\n",
    "            flat = torch.nan_to_num(flat, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        if flat.size(0) != self.state_dim:\n",
    "            padded = torch.zeros(self.state_dim, dtype=torch.float32)\n",
    "            L = min(flat.size(0), self.state_dim)\n",
    "            padded[:L] = flat[:L]\n",
    "        else:\n",
    "            padded = flat\n",
    "        # adjacency  dense, pad, float32..\n",
    "        # if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "        #     warnings.warn(\n",
    "        #         \" Edge attributes found,  but not used in QNN. Adj can be taken as W,\"\n",
    "        #     )\n",
    "            # adj = to_dense_adj(data.edge_index,edge_attr= data.edge_attr, max_num_nodes=data.num_nodes)[0].float()\n",
    "\n",
    "        adj = to_dense_adj(data.edge_index, max_num_nodes=data.num_nodes)[0].float()\n",
    "        padA = torch.zeros(self.max_nodes, self.max_nodes, dtype=torch.float32)\n",
    "        padA[: data.num_nodes, : data.num_nodes] = adj\n",
    "\n",
    "        pm = self.compute_qubit_connections(padA)\n",
    "\n",
    "        # label  long for CrossEntropy\n",
    "        label = data.y.long().squeeze()\n",
    "        return padded, pm, label\n",
    "\n",
    "    def preprocess_dataset(self):\n",
    "        triplets = [self.preprocess_graph(d) for d in self.dataset]\n",
    "        feats, phases, labs = zip(*triplets)\n",
    "        return torch.stack(feats), torch.stack(phases), torch.stack(labs)\n",
    "\n",
    "    # Add method to process subsets\n",
    "    def preprocess_subset(self, subset):\n",
    "        triplets = [self.preprocess_graph(d) for d in subset]\n",
    "        feats, phases, labs = zip(*triplets)\n",
    "        return torch.stack(feats), torch.stack(phases), torch.stack(labs)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for feats, phases, labs in loader:\n",
    "            feats, phases, labs = feats.to(device), phases.to(device), labs.to(device)\n",
    "            logits = model((feats, phases))\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labs).sum().item()\n",
    "            total += labs.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def train_and_eval_fold(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    n_qubits,\n",
    "    num_classes,\n",
    "    dataset_name,\n",
    "    n_layers,\n",
    "    hidden_dims,\n",
    "    lr,\n",
    "    epochs,\n",
    "    seed,\n",
    "    dropout_prob=0.25,\n",
    "    weight_decay=1e-5,\n",
    "    patience=15,\n",
    "):\n",
    "    # reproducibility & device\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # model, loss, optimizer, scheduler\n",
    "    model = QuantumGNN(\n",
    "        n_qubits=n_qubits,\n",
    "        n_layers=n_layers,\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=num_classes,\n",
    "        dropout_prob=dropout_prob,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    print(f\"Using {device} for training\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(\n",
    "        f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
    "    )\n",
    "    print(f\"Optimizer: {optimizer}\")\n",
    "    print(f\"Loss Function: {criterion}\")\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=0.1, min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_epoch = 0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #  train \n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        for feats, phases, labs in train_loader:\n",
    "            feats, phases, labs = (\n",
    "                feats.to(device),\n",
    "                phases.to(device),\n",
    "                labs.to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            logits = model((feats, phases))\n",
    "            loss = criterion(logits, labs)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labs.size(0)\n",
    "            total_samples += labs.size(0)\n",
    "\n",
    "        train_loss = total_loss / total_samples\n",
    "\n",
    "        #  validate \n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        #  early stopping bookkeeping \n",
    "        if (val_acc > best_val_acc) or (\n",
    "            val_acc == best_val_acc and train_loss < best_val_loss\n",
    "        ):\n",
    "            best_val_loss = train_loss\n",
    "            best_val_acc = val_acc\n",
    "            best_val_epoch = epoch\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stop @ epoch {epoch} | Best val_acc={best_val_acc:.4f}\")\n",
    "                break\n",
    "\n",
    "        # optional progress print\n",
    "        # if epoch % 10 == 0 or epoch == 1:\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"[{dataset_name}] Epoch {epoch:03d} | \"\n",
    "            f\"Train Loss {train_loss:.4f} | Val Acc {val_acc:.4f} | LR {lr_now:.1e}\"\n",
    "            f\" [Best Val Acc: {best_val_acc:.4f} @ Epoch {best_val_epoch}]\"\n",
    "        )\n",
    "\n",
    "    #  test \n",
    "    # restore best weights\n",
    "    model.load_state_dict(best_state)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "    print(f\"[{dataset_name}] Fold Test Acc: {test_acc:.4f}\\n\")\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Cross-Validation Function\n",
    "# ========================\n",
    "\n",
    "# assume GraphPreprocessor, QuantumGNN, cross_validate_model,\n",
    "# train_and_eval_fold, evaluate are already defined/imported above\n",
    "\n",
    "\n",
    "def cross_validate_model(\n",
    "    dataset_name=\"MUTAG\",\n",
    "    n_splits=10,\n",
    "    val_ratio=0.1,\n",
    "    n_layers=2,\n",
    "    hidden_dims=[32, 16],\n",
    "    batch_size=16,\n",
    "    lr=1e-3,\n",
    "    epochs=20,\n",
    "    seed=42,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # if dataset_name == \"MNISTSuperpixels\":\n",
    "    #     dataset = MNISTSuperpixels(root=\"data/MNISTSuperpixels\", transform=None)\n",
    "    # elif dataset_name in (\"ESOL\", \"FreeSolv\", \"Lipo\", \"PCBA\", \"MUV\",\n",
    "    #                       \"HIV\", \"BACE\", \"BBBP\", \"Tox21\", \"ToxCast\",\n",
    "    #                       \"SIDER\", \"ClinTox\"):\n",
    "    #     dataset = MoleculeNet(root=\"data/MoleculeNet\", name=dataset_name)\n",
    "    # else:\n",
    "    #\n",
    "    dataset = TUDataset(\n",
    "        root=\"data/TUDataset\",\n",
    "        name=dataset_name,\n",
    "        use_node_attr=True,  # use_edge_attr=False,  # edge attributes not used in this implementation\n",
    "    )\n",
    "    # get a numpy array of all graph labels\n",
    "    labels = dataset.y.cpu().numpy()  #  shape: [num_graphs]\n",
    "\n",
    "    # build an index array to split on\n",
    "    idx = np.arange(len(dataset))  #  [0,1,2,,num_graphs-1]\n",
    "\n",
    "    skf_outer = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fold_accs = []\n",
    "\n",
    "    for fold, (train_val_idx, test_idx) in enumerate(\n",
    "        skf_outer.split(idx, labels), start=1\n",
    "    ):\n",
    "        # carve out small validation set from train_val\n",
    "        tv_labels = labels[train_val_idx]\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            train_val_idx,\n",
    "            test_size=int(len(train_val_idx) * val_ratio),\n",
    "            stratify=tv_labels,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        # build subsets\n",
    "        train_ds = dataset[train_idx.tolist()]\n",
    "        val_ds = dataset[val_idx.tolist()]\n",
    "        test_ds = dataset[test_idx.tolist()]\n",
    "\n",
    "        # preprocess graphs\n",
    "        pre = GraphPreprocessor(dataset)\n",
    "        print(f\"number of qubits {pre.n_qubits}\")\n",
    "        print(f\"number of nodes {pre.max_nodes}\")\n",
    "        print(f\"number of features {pre.feature_dim}\")\n",
    "        train_feats, train_phases, train_labs = pre.preprocess_subset(train_ds)\n",
    "        val_feats, val_phases, val_labs = pre.preprocess_subset(val_ds)\n",
    "        test_feats, test_phases, test_labs = pre.preprocess_subset(test_ds)\n",
    "        print(f\"train_feats shape: {train_feats.shape}\")\n",
    "        print(f\"train_phases shape: {train_phases.shape}\")\n",
    "        print(f\"train_labs shape: {train_labs.shape}\")\n",
    "        # data loaders\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(train_feats, train_phases, train_labs),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            TensorDataset(val_feats, val_phases, val_labs),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            TensorDataset(test_feats, test_phases, test_labs),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # train & evaluate this fold\n",
    "        test_acc = train_and_eval_fold(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            n_qubits=pre.n_qubits,\n",
    "            num_classes=dataset.num_classes,\n",
    "            dataset_name=dataset_name,\n",
    "            n_layers=n_layers,\n",
    "            hidden_dims=hidden_dims,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            seed=seed,\n",
    "        )\n",
    "        fold_accs.append(test_acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_accs)\n",
    "    std_acc = np.std(fold_accs)\n",
    "    print(f\"\\n=== Cross-Validation Results for {dataset_name} ===\")\n",
    "    print(f\"Per-fold Accuracies: {fold_accs}\")\n",
    "    print(f\"Mean Test Acc: {mean_acc:.4f}  {std_acc:.4f}\")\n",
    "    return fold_accs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"dataset_name\": 'COX2',  # Choose from MUTAG, COX2, DHFR, PTC_MR, PTC_FM\n",
    "        \"n_layers\": 4,  # Number of quantum layers\n",
    "        \"hidden_dims\": [32, 16],  # Hidden dimensions for classical head\n",
    "        # \"hidden_dims\": [32,64],  # Hidden dimensions for classical head\n",
    "        # \"hidden_dims\": [256, 128],  # Hidden dimensions for classical head\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 0.01,\n",
    "        \"n_splits\": 10,\n",
    "        \"val_ratio\": 0.1,\n",
    "        \"epochs\": 50,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    print(config)\n",
    "    # Run CV\n",
    "    fold_accuracies = cross_validate_model(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'COIL-DEL', 'n_layers': 4, 'hidden_dims': [32, 64], 'batch_size': 32, 'lr': 0.01, 'n_splits': 10, 'val_ratio': 0.1, 'epochs': 50, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/COIL-DEL.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of qubits 8\n",
      "number of nodes 77\n",
      "number of features 2\n",
      "train_feats shape: torch.Size([3159, 256])\n",
      "train_phases shape: torch.Size([3159, 8, 8])\n",
      "train_labs shape: torch.Size([3159])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 9348\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COIL-DEL] Epoch 001 | Train Loss 4.0705 | Val Acc 0.0826 | LR 1.0e-02 [Best Val Acc: 0.0826 @ Epoch 1]\n",
      "[COIL-DEL] Epoch 002 | Train Loss 3.6112 | Val Acc 0.1168 | LR 1.0e-02 [Best Val Acc: 0.1168 @ Epoch 2]\n",
      "[COIL-DEL] Epoch 003 | Train Loss 3.4000 | Val Acc 0.1795 | LR 1.0e-02 [Best Val Acc: 0.1795 @ Epoch 3]\n",
      "[COIL-DEL] Epoch 004 | Train Loss 3.2815 | Val Acc 0.1681 | LR 1.0e-02 [Best Val Acc: 0.1795 @ Epoch 3]\n",
      "[COIL-DEL] Epoch 005 | Train Loss 3.1830 | Val Acc 0.1823 | LR 1.0e-02 [Best Val Acc: 0.1823 @ Epoch 5]\n",
      "[COIL-DEL] Epoch 006 | Train Loss 3.1069 | Val Acc 0.1880 | LR 1.0e-02 [Best Val Acc: 0.1880 @ Epoch 6]\n",
      "[COIL-DEL] Epoch 007 | Train Loss 3.0218 | Val Acc 0.1681 | LR 1.0e-02 [Best Val Acc: 0.1880 @ Epoch 6]\n",
      "[COIL-DEL] Epoch 008 | Train Loss 3.0190 | Val Acc 0.2194 | LR 1.0e-02 [Best Val Acc: 0.2194 @ Epoch 8]\n",
      "[COIL-DEL] Epoch 009 | Train Loss 2.9566 | Val Acc 0.2080 | LR 1.0e-02 [Best Val Acc: 0.2194 @ Epoch 8]\n",
      "[COIL-DEL] Epoch 010 | Train Loss 2.8951 | Val Acc 0.2137 | LR 1.0e-02 [Best Val Acc: 0.2194 @ Epoch 8]\n",
      "[COIL-DEL] Epoch 011 | Train Loss 2.8720 | Val Acc 0.2536 | LR 1.0e-02 [Best Val Acc: 0.2536 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 012 | Train Loss 2.8473 | Val Acc 0.1909 | LR 1.0e-02 [Best Val Acc: 0.2536 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 013 | Train Loss 2.7977 | Val Acc 0.2308 | LR 1.0e-02 [Best Val Acc: 0.2536 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 014 | Train Loss 2.7607 | Val Acc 0.2251 | LR 1.0e-02 [Best Val Acc: 0.2536 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 015 | Train Loss 2.7889 | Val Acc 0.2422 | LR 1.0e-02 [Best Val Acc: 0.2536 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 016 | Train Loss 2.8099 | Val Acc 0.2792 | LR 1.0e-02 [Best Val Acc: 0.2792 @ Epoch 16]\n",
      "[COIL-DEL] Epoch 017 | Train Loss 2.7404 | Val Acc 0.2564 | LR 1.0e-02 [Best Val Acc: 0.2792 @ Epoch 16]\n",
      "[COIL-DEL] Epoch 018 | Train Loss 2.7202 | Val Acc 0.2621 | LR 1.0e-02 [Best Val Acc: 0.2792 @ Epoch 16]\n",
      "[COIL-DEL] Epoch 019 | Train Loss 2.6838 | Val Acc 0.2877 | LR 1.0e-02 [Best Val Acc: 0.2877 @ Epoch 19]\n",
      "[COIL-DEL] Epoch 020 | Train Loss 2.6821 | Val Acc 0.3191 | LR 1.0e-02 [Best Val Acc: 0.3191 @ Epoch 20]\n",
      "[COIL-DEL] Epoch 021 | Train Loss 2.6581 | Val Acc 0.2849 | LR 1.0e-02 [Best Val Acc: 0.3191 @ Epoch 20]\n",
      "[COIL-DEL] Epoch 022 | Train Loss 2.7011 | Val Acc 0.3077 | LR 1.0e-02 [Best Val Acc: 0.3191 @ Epoch 20]\n",
      "[COIL-DEL] Epoch 023 | Train Loss 2.6480 | Val Acc 0.3219 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 024 | Train Loss 2.6660 | Val Acc 0.2934 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 025 | Train Loss 2.6515 | Val Acc 0.3077 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 026 | Train Loss 2.6362 | Val Acc 0.2963 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 027 | Train Loss 2.6249 | Val Acc 0.2792 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 028 | Train Loss 2.6039 | Val Acc 0.2963 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 029 | Train Loss 2.6054 | Val Acc 0.2735 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 030 | Train Loss 2.6068 | Val Acc 0.3020 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 031 | Train Loss 2.5929 | Val Acc 0.2764 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 032 | Train Loss 2.5597 | Val Acc 0.2650 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 033 | Train Loss 2.5716 | Val Acc 0.3077 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 034 | Train Loss 2.5606 | Val Acc 0.2934 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 035 | Train Loss 2.5665 | Val Acc 0.2934 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 036 | Train Loss 2.5612 | Val Acc 0.2821 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "[COIL-DEL] Epoch 037 | Train Loss 2.5330 | Val Acc 0.2821 | LR 1.0e-02 [Best Val Acc: 0.3219 @ Epoch 23]\n",
      "Early stop @ epoch 38 | Best val_acc=0.3219\n",
      "[COIL-DEL] Fold Test Acc: 0.3410\n",
      "\n",
      "number of qubits 8\n",
      "number of nodes 77\n",
      "number of features 2\n",
      "train_feats shape: torch.Size([3159, 256])\n",
      "train_phases shape: torch.Size([3159, 8, 8])\n",
      "train_labs shape: torch.Size([3159])\n",
      "Using cpu for training\n",
      "Model: QuantumGNN(\n",
      "  (quantum_circuit): QuantumCircuit()\n",
      "  (classical_net): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 9348\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adaskin/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COIL-DEL] Epoch 001 | Train Loss 4.0845 | Val Acc 0.0883 | LR 1.0e-02 [Best Val Acc: 0.0883 @ Epoch 1]\n",
      "[COIL-DEL] Epoch 002 | Train Loss 3.5961 | Val Acc 0.1197 | LR 1.0e-02 [Best Val Acc: 0.1197 @ Epoch 2]\n",
      "[COIL-DEL] Epoch 003 | Train Loss 3.3886 | Val Acc 0.1567 | LR 1.0e-02 [Best Val Acc: 0.1567 @ Epoch 3]\n",
      "[COIL-DEL] Epoch 004 | Train Loss 3.3019 | Val Acc 0.1396 | LR 1.0e-02 [Best Val Acc: 0.1567 @ Epoch 3]\n",
      "[COIL-DEL] Epoch 005 | Train Loss 3.1596 | Val Acc 0.1852 | LR 1.0e-02 [Best Val Acc: 0.1852 @ Epoch 5]\n",
      "[COIL-DEL] Epoch 006 | Train Loss 3.1259 | Val Acc 0.1681 | LR 1.0e-02 [Best Val Acc: 0.1852 @ Epoch 5]\n",
      "[COIL-DEL] Epoch 007 | Train Loss 3.0243 | Val Acc 0.1937 | LR 1.0e-02 [Best Val Acc: 0.1937 @ Epoch 7]\n",
      "[COIL-DEL] Epoch 008 | Train Loss 3.0136 | Val Acc 0.2051 | LR 1.0e-02 [Best Val Acc: 0.2051 @ Epoch 8]\n",
      "[COIL-DEL] Epoch 009 | Train Loss 2.9704 | Val Acc 0.1966 | LR 1.0e-02 [Best Val Acc: 0.2051 @ Epoch 8]\n",
      "[COIL-DEL] Epoch 010 | Train Loss 2.9306 | Val Acc 0.2194 | LR 1.0e-02 [Best Val Acc: 0.2194 @ Epoch 10]\n",
      "[COIL-DEL] Epoch 011 | Train Loss 2.9378 | Val Acc 0.2279 | LR 1.0e-02 [Best Val Acc: 0.2279 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 012 | Train Loss 2.9179 | Val Acc 0.2108 | LR 1.0e-02 [Best Val Acc: 0.2279 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 013 | Train Loss 2.8537 | Val Acc 0.2194 | LR 1.0e-02 [Best Val Acc: 0.2279 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 014 | Train Loss 2.8587 | Val Acc 0.2165 | LR 1.0e-02 [Best Val Acc: 0.2279 @ Epoch 11]\n",
      "[COIL-DEL] Epoch 015 | Train Loss 2.8200 | Val Acc 0.2393 | LR 1.0e-02 [Best Val Acc: 0.2393 @ Epoch 15]\n",
      "[COIL-DEL] Epoch 016 | Train Loss 2.8147 | Val Acc 0.2222 | LR 1.0e-02 [Best Val Acc: 0.2393 @ Epoch 15]\n",
      "[COIL-DEL] Epoch 017 | Train Loss 2.8019 | Val Acc 0.2393 | LR 1.0e-02 [Best Val Acc: 0.2393 @ Epoch 17]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch_geometric.datasets import TUDataset, MNISTSuperpixels, MoleculeNet\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # limit OpenMP threads for Pennylane\n",
    "import pennylane as qml\n",
    "import warnings\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Quantum Circuit Module\n",
    "# ========================\n",
    "class QuantumCircuit(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = 2**n_qubits\n",
    "\n",
    "        # PennyLane device\n",
    "        self.dev = qml.device(\"lightning.qubit\", wires=n_qubits, batch_obs=True)\n",
    "\n",
    "        # count of rotation & QFT params per layer\n",
    "        n_pairs = n_qubits * (n_qubits - 1) // 2\n",
    "        self.rot_params_per_layer = n_qubits + n_pairs\n",
    "        self.qft_params_per_layer = n_pairs\n",
    "\n",
    "        # trainable parameters (float32 by default)\n",
    "        self.ry_thetas = nn.Parameter(torch.rand(n_layers, self.rot_params_per_layer))\n",
    "        self.qft_phases = nn.Parameter(torch.rand(n_layers, self.qft_params_per_layer))\n",
    "\n",
    "        # Torchbacked QNode\n",
    "        self.qnode = qml.QNode(\n",
    "            self.circuit, self.dev, interface=\"torch\", diff_method=\"adjoint\"\n",
    "        )\n",
    "\n",
    "    def rotation_layer(self, thetas, qubit_connections):\n",
    "        idx = 0\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(thetas[idx], wires=i)\n",
    "            idx += 1\n",
    "        for c in range(self.n_qubits):\n",
    "            for t in range(c + 1, self.n_qubits):\n",
    "                if qubit_connections[c, t] != 0:\n",
    "                    angle = thetas[idx]\n",
    "                    qml.CRY(angle, wires=[c, t])\n",
    "                idx += 1\n",
    "\n",
    "    def parameterized_qft(self, phases, qubit_connections):\n",
    "        pidx = 0\n",
    "        for tgt in range(self.n_qubits):\n",
    "            qml.Hadamard(wires=tgt)\n",
    "            for ctrl in range(tgt + 1, self.n_qubits):\n",
    "                if qubit_connections[ctrl, tgt] != 0:\n",
    "                    angle = phases[pidx] * 0.9 + 0.1 * qubit_connections[ctrl, tgt]\n",
    "                    qml.CRZ(angle, wires=[ctrl, tgt])\n",
    "                pidx += 1\n",
    "\n",
    "    def circuit(self, input_state, qubit_connections):\n",
    "        # amplitude embedding of a 2**n_qubits vector\n",
    "        norm = torch.norm(input_state)\n",
    "        if torch.isclose(norm, torch.tensor(0.0)):\n",
    "            warnings.warn(\" Zerovector embedding: input to quantum circuit is 0!\")\n",
    "\n",
    "            qml.AmplitudeEmbedding(\n",
    "                input_state + 1e-8, wires=range(self.n_qubits), normalize=True\n",
    "            )\n",
    "        else:\n",
    "            qml.AmplitudeEmbedding(\n",
    "                input_state, wires=range(self.n_qubits), normalize=True\n",
    "            )\n",
    "\n",
    "        for layer in range(self.n_layers):\n",
    "            self.rotation_layer(self.ry_thetas[layer], qubit_connections)\n",
    "            self.parameterized_qft(self.qft_phases[layer], qubit_connections)\n",
    "\n",
    "        # measure PauliZ on each qubit\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "    def forward(self, input_state, qubit_connections):\n",
    "        # print(input_state)\n",
    "        raw_output = self.qnode(input_state, qubit_connections)\n",
    "        # Normalize and clip outputs to prevent explosion\n",
    "        output = torch.stack(raw_output).float()  # Ensure float32\n",
    "        # print(output)\n",
    "        # output = torch.clamp(output, -1.0, 1.0)  # Constrain to [-1,1]\n",
    "        return output  #\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Quantum GNN Module\n",
    "# ========================\n",
    "class QuantumGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_qubits,\n",
    "        n_layers=1,\n",
    "        hidden_dims=[64, 32],\n",
    "        output_dim=1,\n",
    "        dropout_prob=0.25,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "        # quantum backbone\n",
    "        self.quantum_circuit = QuantumCircuit(n_qubits, n_layers)\n",
    "\n",
    "        # classical head takes exactly n_qubits inputs\n",
    "        layers = []\n",
    "        input_dim = n_qubits\n",
    "        for h in hidden_dims:\n",
    "            layers += [\n",
    "                nn.Linear(input_dim, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_prob),\n",
    "            ]\n",
    "            input_dim = h\n",
    "\n",
    "        # final classifier\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.classical_net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, features):\n",
    "        padded_features, qubit_connections = (\n",
    "            features  # shapes: [B, 2**n_qubits], [B, n_qubits, n_qubits]\n",
    "        )\n",
    "        batch_size = padded_features.size(0)\n",
    "\n",
    "        # run each graph through the QNode\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            oq = self.quantum_circuit(padded_features[i], qubit_connections[i])\n",
    "            # stack the n_qubit expectation values  [n_qubits]\n",
    "            # quantum_outputs.append(torch.stack(oq))\n",
    "            quantum_outputs.append(oq)\n",
    "\n",
    "        #  [B, n_qubits] but ensure float32\n",
    "        probs = torch.stack(quantum_outputs, dim=0).float()\n",
    "\n",
    "        # feed classical head  logits\n",
    "        return self.classical_net(probs)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Data Preprocessing\n",
    "# ========================\n",
    "class GraphPreprocessor:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.max_nodes = max(d.num_nodes for d in dataset)\n",
    "        self.feature_dim = dataset.num_node_features\n",
    "        if self.feature_dim == 0:\n",
    "            self.feature_dim = 1\n",
    "        self.n_qubits = math.ceil(math.log2(self.max_nodes * self.feature_dim))\n",
    "        self.state_dim = 2**self.n_qubits\n",
    "        self.bin_repr = self.create_bin_repr()  # [max_nodes, n_qubits] float32\n",
    "\n",
    "    def create_bin_repr(self):\n",
    "        nodes = torch.arange(self.max_nodes)\n",
    "        # bit-shift into n_qubits columns, then float32\n",
    "        br = (nodes.unsqueeze(1) >> torch.arange(self.n_qubits - 1, -1, -1)) & 1\n",
    "        return br.float()\n",
    "\n",
    "    # def compute_qubit_connections(self, adj):\n",
    "    #     # adj: [max_nodes, max_nodes] float32, no selfloops\n",
    "    #     a = adj.clone() / adj.shape[0]  # normalize by node count\n",
    "\n",
    "    #     a.fill_diagonal_(0)\n",
    "    #     #  [n_qubits, n_qubits]\n",
    "    #     return torch.einsum(\"ij,ic,jt->ct\", a, self.bin_repr, self.bin_repr)\n",
    "\n",
    "    # MORE ROBUST PHASE INITIALIZATION\n",
    "    def compute_qubit_connections(\n",
    "        self, adj_matrix, n_qubits=None, base_phase=0.1, noise=0.01\n",
    "    ):\n",
    "        N = adj_matrix.shape[0]\n",
    "        if n_qubits is None:\n",
    "            n_qubits = self.n_qubits\n",
    "\n",
    "        # Use precomputed binary representation\n",
    "        node_bin = self.bin_repr[:N].to(adj_matrix.device)  # Only use first N nodes\n",
    "\n",
    "        # Create off-diagonal mask\n",
    "        off_diag_mask = ~torch.eye(N, dtype=torch.bool, device=adj_matrix.device)\n",
    "\n",
    "        # Prepare weight matrix\n",
    "        W = adj_matrix / N\n",
    "        zero_off_diag = (W == 0) & off_diag_mask\n",
    "        W = torch.where(zero_off_diag, base_phase, W)\n",
    "        W = W * off_diag_mask.float()  # Zero out diagonal\n",
    "\n",
    "        # Compute phase matrix using vectorized operations\n",
    "        phase_matrix = node_bin.t() @ (W @ node_bin)\n",
    "\n",
    "        # Add noise to break symmetry\n",
    "        phase_matrix += noise * torch.randn_like(phase_matrix)\n",
    "\n",
    "        return phase_matrix\n",
    "\n",
    "    def preprocess_graph(self, data):\n",
    "        # flatten & pad\n",
    "        # 1) detect missing or zero-width features\n",
    "        if (\n",
    "            (not hasattr(data, \"x\"))\n",
    "            or data.x == None\n",
    "            or data.x.numel() == 0\n",
    "            or data.x.size(1) == 0\n",
    "        ):\n",
    "            \n",
    "            # ensure you modify the same Data object\n",
    "            warnings.warn(\" No node features found, using ones.\")\n",
    "            data.x = torch.ones(self.state_dim, 1, dtype=torch.float32)\n",
    "\n",
    "        # 2) flatten and pad\n",
    "        flat = data.x.reshape(-1).float()  # length = num_nodes*feat_dim\n",
    "\n",
    "        # Check for NaN/Inf in features\n",
    "        if torch.isnan(flat).any() or torch.isinf(flat).any():\n",
    "            flat = torch.nan_to_num(flat, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        if flat.size(0) != self.state_dim:\n",
    "            padded = torch.zeros(self.state_dim, dtype=torch.float32)\n",
    "            L = min(flat.size(0), self.state_dim)\n",
    "            padded[:L] = flat[:L]\n",
    "        else:\n",
    "            padded = flat\n",
    "        # adjacency  dense, pad, float32..\n",
    "        # if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "        #     warnings.warn(\n",
    "        #         \" Edge attributes found,  but not used in QNN. Adj can be taken as W,\"\n",
    "        #     )\n",
    "            # adj = to_dense_adj(data.edge_index,edge_attr= data.edge_attr, max_num_nodes=data.num_nodes)[0].float()\n",
    "\n",
    "        adj = to_dense_adj(data.edge_index, max_num_nodes=data.num_nodes)[0].float()\n",
    "        padA = torch.zeros(self.max_nodes, self.max_nodes, dtype=torch.float32)\n",
    "        padA[: data.num_nodes, : data.num_nodes] = adj\n",
    "\n",
    "        pm = self.compute_qubit_connections(padA)\n",
    "\n",
    "        # label  long for CrossEntropy\n",
    "        label = data.y.long().squeeze()\n",
    "        return padded, pm, label\n",
    "\n",
    "    def preprocess_dataset(self):\n",
    "        triplets = [self.preprocess_graph(d) for d in self.dataset]\n",
    "        feats, phases, labs = zip(*triplets)\n",
    "        return torch.stack(feats), torch.stack(phases), torch.stack(labs)\n",
    "\n",
    "    # Add method to process subsets\n",
    "    def preprocess_subset(self, subset):\n",
    "        triplets = [self.preprocess_graph(d) for d in subset]\n",
    "        feats, phases, labs = zip(*triplets)\n",
    "        return torch.stack(feats), torch.stack(phases), torch.stack(labs)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for feats, phases, labs in loader:\n",
    "            feats, phases, labs = feats.to(device), phases.to(device), labs.to(device)\n",
    "            logits = model((feats, phases))\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labs).sum().item()\n",
    "            total += labs.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def train_and_eval_fold(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    n_qubits,\n",
    "    num_classes,\n",
    "    dataset_name,\n",
    "    n_layers,\n",
    "    hidden_dims,\n",
    "    lr,\n",
    "    epochs,\n",
    "    seed,\n",
    "    dropout_prob=0.25,\n",
    "    weight_decay=1e-5,\n",
    "    patience=15,\n",
    "):\n",
    "    # reproducibility & device\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # model, loss, optimizer, scheduler\n",
    "    model = QuantumGNN(\n",
    "        n_qubits=n_qubits,\n",
    "        n_layers=n_layers,\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=num_classes,\n",
    "        dropout_prob=dropout_prob,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    print(f\"Using {device} for training\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(\n",
    "        f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
    "    )\n",
    "    print(f\"Optimizer: {optimizer}\")\n",
    "    print(f\"Loss Function: {criterion}\")\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=0.1, min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_epoch = 0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        #  train \n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        for feats, phases, labs in train_loader:\n",
    "            feats, phases, labs = (\n",
    "                feats.to(device),\n",
    "                phases.to(device),\n",
    "                labs.to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            logits = model((feats, phases))\n",
    "            loss = criterion(logits, labs)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labs.size(0)\n",
    "            total_samples += labs.size(0)\n",
    "\n",
    "        train_loss = total_loss / total_samples\n",
    "\n",
    "        #  validate \n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        #  early stopping bookkeeping \n",
    "        if (val_acc > best_val_acc) or (\n",
    "            val_acc == best_val_acc and train_loss < best_val_loss\n",
    "        ):\n",
    "            best_val_loss = train_loss\n",
    "            best_val_acc = val_acc\n",
    "            best_val_epoch = epoch\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stop @ epoch {epoch} | Best val_acc={best_val_acc:.4f}\")\n",
    "                break\n",
    "\n",
    "        # optional progress print\n",
    "        # if epoch % 10 == 0 or epoch == 1:\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"[{dataset_name}] Epoch {epoch:03d} | \"\n",
    "            f\"Train Loss {train_loss:.4f} | Val Acc {val_acc:.4f} | LR {lr_now:.1e}\"\n",
    "            f\" [Best Val Acc: {best_val_acc:.4f} @ Epoch {best_val_epoch}]\"\n",
    "        )\n",
    "\n",
    "    #  test \n",
    "    # restore best weights\n",
    "    model.load_state_dict(best_state)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "    print(f\"[{dataset_name}] Fold Test Acc: {test_acc:.4f}\\n\")\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Cross-Validation Function\n",
    "# ========================\n",
    "\n",
    "# assume GraphPreprocessor, QuantumGNN, cross_validate_model,\n",
    "# train_and_eval_fold, evaluate are already defined/imported above\n",
    "\n",
    "\n",
    "def cross_validate_model(\n",
    "    dataset_name=\"MUTAG\",\n",
    "    n_splits=10,\n",
    "    val_ratio=0.1,\n",
    "    n_layers=2,\n",
    "    hidden_dims=[32, 16],\n",
    "    batch_size=16,\n",
    "    lr=1e-3,\n",
    "    epochs=20,\n",
    "    seed=42,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # if dataset_name == \"MNISTSuperpixels\":\n",
    "    #     dataset = MNISTSuperpixels(root=\"data/MNISTSuperpixels\", transform=None)\n",
    "    # elif dataset_name in (\"ESOL\", \"FreeSolv\", \"Lipo\", \"PCBA\", \"MUV\",\n",
    "    #                       \"HIV\", \"BACE\", \"BBBP\", \"Tox21\", \"ToxCast\",\n",
    "    #                       \"SIDER\", \"ClinTox\"):\n",
    "    #     dataset = MoleculeNet(root=\"data/MoleculeNet\", name=dataset_name)\n",
    "    # else:\n",
    "    #\n",
    "    dataset = TUDataset(\n",
    "        root=\"data/TUDataset\",\n",
    "        name=dataset_name,\n",
    "        use_node_attr=True,  # use_edge_attr=False,  # edge attributes not used in this implementation\n",
    "    )\n",
    "    # get a numpy array of all graph labels\n",
    "    labels = dataset.y.cpu().numpy()  #  shape: [num_graphs]\n",
    "\n",
    "    # build an index array to split on\n",
    "    idx = np.arange(len(dataset))  #  [0,1,2,,num_graphs-1]\n",
    "\n",
    "    skf_outer = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fold_accs = []\n",
    "\n",
    "    for fold, (train_val_idx, test_idx) in enumerate(\n",
    "        skf_outer.split(idx, labels), start=1\n",
    "    ):\n",
    "        # carve out small validation set from train_val\n",
    "        tv_labels = labels[train_val_idx]\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            train_val_idx,\n",
    "            test_size=int(len(train_val_idx) * val_ratio),\n",
    "            stratify=tv_labels,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        # build subsets\n",
    "        train_ds = dataset[train_idx.tolist()]\n",
    "        val_ds = dataset[val_idx.tolist()]\n",
    "        test_ds = dataset[test_idx.tolist()]\n",
    "\n",
    "        # preprocess graphs\n",
    "        pre = GraphPreprocessor(dataset)\n",
    "        print(f\"number of qubits {pre.n_qubits}\")\n",
    "        print(f\"number of nodes {pre.max_nodes}\")\n",
    "        print(f\"number of features {pre.feature_dim}\")\n",
    "        train_feats, train_phases, train_labs = pre.preprocess_subset(train_ds)\n",
    "        val_feats, val_phases, val_labs = pre.preprocess_subset(val_ds)\n",
    "        test_feats, test_phases, test_labs = pre.preprocess_subset(test_ds)\n",
    "        print(f\"train_feats shape: {train_feats.shape}\")\n",
    "        print(f\"train_phases shape: {train_phases.shape}\")\n",
    "        print(f\"train_labs shape: {train_labs.shape}\")\n",
    "        # data loaders\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(train_feats, train_phases, train_labs),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            TensorDataset(val_feats, val_phases, val_labs),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            TensorDataset(test_feats, test_phases, test_labs),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # train & evaluate this fold\n",
    "        test_acc = train_and_eval_fold(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            n_qubits=pre.n_qubits,\n",
    "            num_classes=dataset.num_classes,\n",
    "            dataset_name=dataset_name,\n",
    "            n_layers=n_layers,\n",
    "            hidden_dims=hidden_dims,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            seed=seed,\n",
    "        )\n",
    "        fold_accs.append(test_acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_accs)\n",
    "    std_acc = np.std(fold_accs)\n",
    "    print(f\"\\n=== Cross-Validation Results for {dataset_name} ===\")\n",
    "    print(f\"Per-fold Accuracies: {fold_accs}\")\n",
    "    print(f\"Mean Test Acc: {mean_acc:.4f}  {std_acc:.4f}\")\n",
    "    return fold_accs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"dataset_name\": 'COIL-DEL',  # Choose from MUTAG, COX2, DHFR, PTC_MR, PTC_FM\n",
    "        \"n_layers\": 4,  # Number of quantum layers\n",
    "        \"hidden_dims\": [32, 64],  # Hidden dimensions for classical head\n",
    "        # \"hidden_dims\": [32,64],  # Hidden dimensions for classical head\n",
    "        # \"hidden_dims\": [256, 128],  # Hidden dimensions for classical head\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 0.01,\n",
    "        \"n_splits\": 10,\n",
    "        \"val_ratio\": 0.1,\n",
    "        \"epochs\": 50,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    print(config)\n",
    "    # Run CV\n",
    "    fold_accuracies = cross_validate_model(**config)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
